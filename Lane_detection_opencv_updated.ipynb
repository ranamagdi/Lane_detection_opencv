{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import os\n",
    "import pickle #use it to save the calibration\n",
    "import re #Regular expression operations\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some usefull functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryint(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return s\n",
    "    \n",
    "#This function to turn a string into a list of string and number chunks like:\"z23a\" -> [\"z\", 23, \"a\"]    \n",
    "def alphanum_key(s):\n",
    "\n",
    "    return [tryint(c) for c in re.split('([0-9]+)', s)]\n",
    "\n",
    "#This function to Sort the given list\n",
    "def sort_nicely(l):\n",
    "    \n",
    "    l.sort(key=alphanum_key)\n",
    "    \n",
    "def plot_images(data, layout='row', cols=2, figsize=(20, 12)):\n",
    "    '''\n",
    "   This function for plotting images\n",
    "    :parameter data [(ndarray, string)]: List of data to display, [(image, title)]\n",
    "    :parameter layout (string): Layout, row-wise or column-wise\n",
    "    :parameter cols (number): Number of columns per row\n",
    "    :parameter figsize (number, number): Tuple indicating figure size\n",
    "    '''\n",
    "    rows = math.ceil(len(data) / cols)\n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "    if layout == 'row':\n",
    "        for idx, d in enumerate(data):\n",
    "            img, title = d\n",
    "\n",
    "            plt.subplot(rows, cols, idx+1)\n",
    "            plt.title(title, fontsize=20)\n",
    "            plt.axis('off')\n",
    "            if len(img.shape) == 2:\n",
    "                plt.imshow(img, cmap='gray')\n",
    "                \n",
    "            elif len(img.shape) == 3:\n",
    "                plt.imshow(img)\n",
    "                \n",
    "    elif layout == 'col':\n",
    "        counter = 0\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                img, title = data[r + rows*c]\n",
    "                nb_channels = len(img.shape)\n",
    "                \n",
    "                plt.subplot(rows, cols, counter+1)\n",
    "                plt.title(title, fontsize=20)\n",
    "                plt.axis('off')\n",
    "                if len(img.shape) == 2:\n",
    "                    plt.imshow(img, cmap='gray')\n",
    "                \n",
    "                elif len(img.shape) == 3:\n",
    "                    plt.imshow(img)\n",
    "                    counter += 1\n",
    "  \n",
    "    return ax    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Get the shape of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image is of shape (720, 1280, 3)\n"
     ]
    }
   ],
   "source": [
    "def show_image(image, title='Image', cmap_type='gray'):\n",
    "    plt.imshow(image, cmap = cmap_type)\n",
    "    plt.title(title)\n",
    "test_img_paths = glob.glob('Project_data/test_images/test*.jpg')\n",
    "sort_nicely(test_img_paths)\n",
    "images = [plt.imread(image) for image in test_img_paths]\n",
    "\n",
    "# getting img dimensions and printing it\n",
    "image_idx = 0\n",
    "\n",
    "print(\"This image is of shape {}\".format(images[image_idx].shape))\n",
    "show_image(images[image_idx], \"Image number one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get capture video frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "  '''\n",
    "    This function that captures and stores video frames\n",
    "    :param video_path (string): Video path\n",
    "    :param frames_dir (string): Frames directory\n",
    "    '''\n",
    "#Note: this function add it to test more in the \"challenge_video.mp4\"\n",
    "def capture_frames(video_path, frames_dir):\n",
    "  \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    print('Starting frame capture...')\n",
    "    \n",
    "    count = 0\n",
    "    success = True\n",
    "    while success:\n",
    "        success, frame = cap.read()\n",
    "        cv2.imwrite(frames_dir + 'frame{:02}.jpg'.format(count), frame)\n",
    "        count += 1\n",
    "\n",
    "    print('Completed!')\n",
    "    \n",
    "video1 = glob.glob('video_frames/frame*.jpg')\n",
    "sort_nicely(video1)\n",
    "\n",
    "video2 = glob.glob('video_frames_1/frame*.jpg')\n",
    "sort_nicely(video2)\n",
    "\n",
    "# List of all demos to visualise(test images from 0-7)\n",
    "plot_demo = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Camera Calibration & Distortion correction\n",
    "In the \"old_version_with_output/Lane_detection_opencv\" notebook we didn't make this step and we get output fail to detect curved lanes accurately, and was not robust to obstructions and shadows so we cann't measure the curve in accurate direction.So we read about this step and why we need it! and we read that the Camera lenses distort incoming light to focus it on the camera sensor. Although this is very useful in allowing us to capture images of our environment, they often end up distorting light slightly inaccurately. This can result in inaccurate measurements in computer vision applications. so , we need to correct this to get more improvement.so how we can do this?..you can calibrate your image against a known object, and generate a distortion model which accounts for lens distortions. This object is often an asymmetric checkerboard.The steps to do that: We begin by converting the image to grayscale, then applying the cv2.findChessboardCorners() function. We already know that this chessboard is a 2 dimensional object with exclusively straight lines, so we can apply some transformations to the detected corners to align them properly. I used the cv2.CalibrateCamera() to get the distortion coefficients and the camera matrix. The camera has been calibrated!You can then use cv2.undistort() to correct the rest of your input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the saved camera calibration matrix & dist coefficients!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calibrate_camera():\n",
    "    '''\n",
    "    Computes the camera calibration matrix and distortion coefficients\n",
    "    :return: Camera calibration matrix and distortion coefficients\n",
    "    '''\n",
    "    \n",
    "    imgpaths = glob.glob('Project_data/camera_cal/calibration*.jpg')\n",
    "    sort_nicely(imgpaths)\n",
    "    \n",
    "    # View a sample calibration image\n",
    "    %matplotlib inline\n",
    "    \n",
    "    image = cv2.imread(imgpaths[0])\n",
    "    imshape = image.shape[:2] # gets only the (height, width) to be used in the cv2.calibrateCamera()\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    print('Image shape: {}'.format(image.shape))\n",
    "\n",
    "    %matplotlib qt\n",
    "    print()\n",
    "    print('Calibrating the camera...')\n",
    "    print()\n",
    "    objpoints = []\n",
    "    imgpoints = []\n",
    "\n",
    "    nx = 9 # Number of inside corners on each row of the chessboard\n",
    "    ny = 6 # Number of inside corners on each column of the chessboard\n",
    "\n",
    "    # Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros([ny*nx, 3], dtype=np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1, 2)\n",
    "\n",
    "    # Iterate over each calibration image and determine the objpoints and imgpoints\n",
    "    for idx, imgpath in enumerate(imgpaths):\n",
    "        img = cv2.imread(imgpath)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "        if ret:\n",
    "            img = cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "\n",
    "            imgpoints.append(corners)\n",
    "            objpoints.append(objp)\n",
    "\n",
    "            cv2.imshow('img', img)\n",
    "            cv2.waitKey(500)\n",
    "\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, imshape[::-1], None, None)\n",
    "   \n",
    "    print('Calibration complete!')\n",
    "    cv2.destroyAllWindows()\n",
    "    return mtx, dist\n",
    "# Note: the calibration process only needs to be run once in the absense of the pickled file\n",
    "# containing the calculated aforementioned params\n",
    "if os.path.exists('camera_calib.p'):\n",
    "    with open('camera_calib.p', mode='rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        mtx, dist = data['mtx'], data['dist']\n",
    "        print('Loaded the saved camera calibration matrix & dist coefficients!')\n",
    "else:\n",
    "    mtx, dist = calibrate_camera()\n",
    "    with open('camera_calib.p', mode='wb') as f:\n",
    "        pickle.dump({'mtx': mtx, 'dist': dist}, f)\n",
    "\n",
    "def undistort(img, mtx, dist):\n",
    "    '''\n",
    "    Undistorts an image\n",
    "    :param img (ndarray): Image, represented an a numpy array\n",
    "    :param mtx: Camera calibration matrix\n",
    "    :param dist: Distortion coeff's\n",
    "    :return : Undistorted image\n",
    "    '''\n",
    "    \n",
    "    undistort = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undistort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Demo \n",
    "Note:\"The figures will show in external when run the cell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undistort a sample camera calibration image and a sample test image\n",
    "\n",
    "if 1 in plot_demo:\n",
    "    ccimg = cv2.imread('Project_data/camera_cal/calibration1.jpg')\n",
    "    ccimg_undist = undistort(ccimg, mtx, dist)\n",
    "\n",
    "    plot_images([\n",
    "       (ccimg, 'Original Image'),\n",
    "       (ccimg_undist, 'Undistorted Image')\n",
    "    ])\n",
    "    \n",
    "    img_orig = mpimg.imread(test_img_paths[2])\n",
    "    img = undistort(img_orig, mtx, dist)\n",
    "    \n",
    "    plot_images([\n",
    "        (img_orig, 'Original Image'),\n",
    "        (img, 'Undistorted Image')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Perspective Transformation\n",
    "Following the distortion correction, an undistorted image undergoes Perspective Transformation which warpes the image into a bird's eye view scene. This makes it easier to detect the lane lines (since they are relatively parallel) and measure their curvature.\n",
    "\n",
    "Firstly, we compute the transformation matrix by passing the src and dst points into cv2.getPerspectiveTransform. These points are determined empirically with the help of the suite of test images.\n",
    "Then, the undistorted image is warped by passing it into cv2.warpPerspective along with the transformation matrix\n",
    "Finally, we cut/crop out the sides of the image using a function get_roi() since this portion of the image contains no relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (720, 1280)\n",
    "\n",
    "def get_roi(img, vertices):\n",
    "    '''\n",
    "    Transforms an image by preserving only the ROI \"region of interst\" represented by the\n",
    "    the 'vertices' and removes the remainder of the image by setting the pixel intensity to 0\n",
    "    :parameter img (ndarray): Image\n",
    "    :parameter vertices (ndarray): Region of Interest of the image\n",
    "    :return : Modified image\n",
    "    '''\n",
    "    \n",
    "    vertices = np.array(vertices, ndmin=3, dtype=np.int32)\n",
    "    if len(img.shape) == 3:\n",
    "        fill_color = (255,) * 3\n",
    "    else:\n",
    "        fill_color = 255\n",
    "            \n",
    "    mask = np.zeros_like(img)\n",
    "    mask = cv2.fillPoly(mask, vertices, fill_color)\n",
    "    return cv2.bitwise_and(img, mask)\n",
    "\n",
    "def warp_image(img, warp_shape, src, dst):\n",
    "    '''\n",
    "    Performs perspective transformation (PT)\n",
    "    :parameter img (ndarray): Image\n",
    "    :parameter warp_shape: Shape of the warped image\n",
    "    :parameter src (ndarray): Source points\n",
    "    :parameter dst (ndarray): Destination points\n",
    "    :return : Tuple (Transformed image, PT matrix, PT inverse matrix)\n",
    "    '''\n",
    "    \n",
    "    # Get the perspective transformation matrix and its inverse\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    invM = cv2.getPerspectiveTransform(dst, src)\n",
    "    \n",
    "    # Warp the image\n",
    "    warped = cv2.warpPerspective(img, M, warp_shape, flags=cv2.INTER_LINEAR)\n",
    "    return warped, M, invM\n",
    "\n",
    "### Apply gaussian blue to eliminate noise\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def preprocess_image(img, visualise=False):\n",
    "    kernel_size=3\n",
    "    '''\n",
    "    Pre-processes an image. Steps include:\n",
    "    1. Distortion correction\n",
    "    2. Perspective Transformation\n",
    "    3. ROI crop\n",
    "    \n",
    "    :parameter img (ndarray): Original Image\n",
    "    :parameter visualise (boolean): Boolean flag for visualisation\n",
    "    :return : Pre-processed image, (PT matrix, PT inverse matrix)\n",
    "    '''\n",
    "    \n",
    "    ysize = img.shape[0]\n",
    "    xsize = img.shape[1]\n",
    "    \n",
    "    # 1. eliminate noise\n",
    "    gaussian_blur(img, kernel_size)\n",
    "    \n",
    "    # 2. Distortion correction\n",
    "    undist = undistort(img, mtx, dist)\n",
    "    \n",
    "    # 2. Perspective transformation\n",
    "    src = np.float32([\n",
    "        (696,455),    \n",
    "        (587,455), \n",
    "        (235,700),  \n",
    "        (1075,700)\n",
    "    ])\n",
    "    dst = np.float32([\n",
    "        (xsize - 350, 0),\n",
    "        (350, 0),\n",
    "        (350, ysize),\n",
    "        (xsize - 350, ysize)\n",
    "    ])\n",
    "\n",
    "    warped, M, invM = warp_image(undist, (xsize, ysize), src, dst)\n",
    "\n",
    "    # 3. ROI crop\n",
    "    vertices = np.array([\n",
    "        [200, ysize],\n",
    "        [200, 0],\n",
    "        [1100, 0],\n",
    "        [1100, ysize]\n",
    "    ])\n",
    "\n",
    "    roi = get_roi(warped, vertices)\n",
    "\n",
    "    # 4. Visualise the transformation\n",
    "    if visualise:\n",
    "        img_copy = np.copy(img)\n",
    "        roi_copy = np.copy(roi)\n",
    "        \n",
    "        cv2.polylines(img_copy, [np.int32(src)], True, (255, 0, 0), 3)\n",
    "        cv2.polylines(roi_copy, [np.int32(dst)], True, (255, 0, 0), 3)\n",
    "        \n",
    "        plot_images([\n",
    "            (img_copy, 'Original Image'),\n",
    "            (roi_copy, 'Bird\\'s Eye View Perspective')\n",
    "        ])\n",
    "\n",
    "    return roi, (M, invM)\n",
    "\n",
    "def get_image(img_path, visualise=False):\n",
    "    '''\n",
    "    Load an image from the 'img_path' and pre-process it\n",
    "    :parameter img_path (string): Image path\n",
    "    :parameter visualise (boolean): Boolean flag for visualisation\n",
    "    :return : Transformed Image, (PT matrix, PT inv matrix)\n",
    "    '''\n",
    "    img = mpimg.imread(img_path)\n",
    "    return preprocess_image(img, visualise=visualise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Demo\n",
    "Note:\"The figures will show in external when run the cell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 2 in plot_demo:\n",
    "    for path in test_img_paths[:]:\n",
    "        get_image(path, visualise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Generate Thresholded Binary image\n",
    "Many techniques such as gradient thresholding, thresholding over individual colour channels of different color spaces and a combination of them were experimented with over a training set of images with the aim of best filtering the lane line pixels from other pixels. The experimentation yielded the following key insights:\n",
    "\n",
    "1.The performance of indvidual color channels varied in detecting the two colors (white and yellow) with some transforms significantly outperforming the others in detecting one color but showcasing poor performance when employed for detecting the other. Out of all the channels of RGB, HLS, HSV and LAB color spaces that were experiemented with the below mentioned provided the greatest signal-to-noise ratio and robustness against varying lighting conditions:\n",
    "\n",
    "White pixel detection: R-channel (RGB) and L-channel (HLS)\n",
    "Yellow pixel detection: B-channel (LAB) and S-channel (HLS)\n",
    "2.Owing to the uneven road surfaces and non-uniform lighting conditions a strong need for Adaptive Thresholding was realised\n",
    "\n",
    "3.Gradient thresholding didn't provide any performance improvements over the color thresholding methods employed above, and hence, it was not used in the pipeline.\n",
    "\n",
    "The final solution used in the pipeline consisted of an ensemble of threshold masks. Some of the key callout points are:\n",
    "\n",
    "Five masks were used, namely, RGB, HLS, HSV, LAB and a custom adaptive mask\n",
    "\n",
    "Each of these masks were composed through a Logical OR of two sub-masks created to detect the two lane line colors of yellow and white. Moreover, the threshold values associated with each sub-mask was adaptive to the mean of image / search window (further details on the search window has been provided in the sub-sections below)\n",
    "\n",
    "Logically, this can explained as: Mask = Sub-mask (white) | Sub-mask (yellow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_threshold(img, low, high):    \n",
    "    if len(img.shape) == 2:\n",
    "        output = np.zeros_like(img)\n",
    "        mask = (img >= low) & (img <= high)\n",
    "        \n",
    "    elif len(img.shape) == 3:\n",
    "        output = np.zeros_like(img[:,:,0])\n",
    "        mask = (img[:,:,0] >= low[0]) & (img[:,:,0] <= high[0]) \\\n",
    "            & (img[:,:,1] >= low[1]) & (img[:,:,1] <= high[1]) \\\n",
    "            & (img[:,:,2] >= low[2]) & (img[:,:,2] <= high[2])\n",
    "            \n",
    "    output[mask] = 1\n",
    "    return output\n",
    "\n",
    "def get_binary_image(img, visualise=False):\n",
    "    \"\"\"\n",
    "    Generate a thresholded binary image using transforms from an ensemble of color spaces: \n",
    "    LAB (Yellow), HSV (Yellow + White), HLS (Yellow + White), RGB (White) and \n",
    "    Adaptive Thresholding ()\n",
    "    :param img (ndarray): Warped image\n",
    "    :param visualise (boolean): Boolean flag for visualisation\n",
    "    :return (ndarray): Thresholded binary image\n",
    "    \"\"\"\n",
    "    \n",
    "    ### LAB color space\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    L = lab[:,:,0]\n",
    "    L_max, L_mean = np.max(L), np.mean(L)\n",
    "    B = lab[:,:,2]\n",
    "    B_max, B_mean = np.max(B), np.mean(B)\n",
    "\n",
    "    # YELLOW\n",
    "    L_adapt_yellow = max(80, int(L_max * 0.45))\n",
    "    B_adapt_yellow =  max(int(B_max * 0.70), int(B_mean * 1.2))\n",
    "    lab_low_yellow = np.array((L_adapt_yellow, 120, B_adapt_yellow))\n",
    "    lab_high_yellow = np.array((255, 145, 255))\n",
    "\n",
    "    lab_yellow = binary_threshold(lab, lab_low_yellow, lab_high_yellow)\n",
    "    lab_binary =lab_yellow\n",
    "      \n",
    "    ### HSV color space\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    H = hsv[:,:,0]\n",
    "    H_max, H_mean = np.max(H), np.mean(H)\n",
    "    S = hsv[:,:,1]\n",
    "    S_max, S_mean = np.max(S), np.mean(S)\n",
    "    V = hsv[:,:,2]\n",
    "    V_max, V_mean = np.max(V), np.mean(V)\n",
    "    \n",
    "    # YELLOW\n",
    "    S_adapt_yellow =  max(int(S_max * 0.25), int(S_mean * 1.75))\n",
    "    V_adapt_yellow =  max(50, int(V_mean * 1.25))\n",
    "    hsv_low_yellow = np.array((15, S_adapt_yellow, V_adapt_yellow))\n",
    "   \n",
    "    hsv_high_yellow = np.array((30, 255, 255))\n",
    "    hsv_yellow = binary_threshold(hsv, hsv_low_yellow, hsv_high_yellow)    \n",
    "\n",
    "    # WHITE\n",
    "    V_adapt_white = max(150, int(V_max * 0.8),int(V_mean * 1.25))\n",
    "    hsv_low_white = np.array((0, 0, V_adapt_white))\n",
    "    hsv_high_white = np.array((255, 40, 220))\n",
    "\n",
    "    hsv_white = binary_threshold(hsv, hsv_low_white, hsv_high_white)\n",
    "\n",
    "    hsv_binary = hsv_yellow | hsv_white\n",
    "\n",
    "    ### HLS color space\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    L = hls[:,:,1]\n",
    "    L_max, L_mean = np.max(L), np.mean(L)\n",
    "    S = hls[:,:,2]\n",
    "    S_max, S_mean = np.max(S), np.mean(S)\n",
    " \n",
    "    # YELLOW\n",
    "    L_adapt_yellow = max(80, int(L_mean * 1.25))\n",
    "    S_adapt_yellow = max(int(S_max * 0.25), int(S_mean * 1.75))\n",
    "    hls_low_yellow = np.array((15, L_adapt_yellow, S_adapt_yellow))\n",
    "    hls_high_yellow = np.array((30, 255, 255))\n",
    "\n",
    "    hls_yellow = binary_threshold(hls, hls_low_yellow, hls_high_yellow)\n",
    "    # WHITE\n",
    "    L_adapt_white =  max(160, int(L_max *0.8),int(L_mean * 1.25))\n",
    "    hls_low_white = np.array((0, L_adapt_white,  0))\n",
    "    hls_high_white = np.array((255, 255, 255))\n",
    "\n",
    "    hls_white = binary_threshold(hls, hls_low_white, hls_high_white)\n",
    "        \n",
    "    hls_binary = hls_yellow | hls_white\n",
    "\n",
    "    ### R color channel (WHITE)\n",
    "    R = img[:,:,0]\n",
    "    R_max, R_mean = np.max(R), np.mean(R)\n",
    "    \n",
    "    R_low_white = min(max(150, int(R_max * 0.55), int(R_mean * 1.95)),230)\n",
    "    R_binary = binary_threshold(R, R_low_white, 255)\n",
    "    \n",
    "    ### Adaptive thresholding: Gaussian kernel \n",
    "    # YELLOW\n",
    "    \n",
    "    adapt_yellow_S = cv2.adaptiveThreshold(hls[:,:,2], 1, \\\n",
    "                                                  cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 161, -5)\n",
    "    adapt_yellow_B = cv2.adaptiveThreshold(lab[:,:,2], 1, \\\n",
    "                                                  cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 161, -5)\n",
    "    adapt_yellow = adapt_yellow_S & adapt_yellow_B\n",
    "    \n",
    "    # WHITE\n",
    "    adapt_white_R = cv2.adaptiveThreshold(img[:,:,0], 1, \\\n",
    "                                                  cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 161, -27)\n",
    "    adapt_white_L = cv2.adaptiveThreshold(hsv[:,:,2], 1, \\\n",
    "                                                  cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 161, -27)\n",
    "    adapt_white = adapt_white_R & adapt_white_L   \n",
    "                                                  \n",
    "    adapt_binary =  adapt_yellow | adapt_white\n",
    "    \n",
    "    ### Ensemble Voting\n",
    "    combined = np.asarray(R_binary + lab_binary + hls_binary + hsv_binary + adapt_binary, dtype=np.uint8)\n",
    "    combined[combined < 3] = 0\n",
    "    combined[combined >= 3] = 1\n",
    "\n",
    "    if visualise:\n",
    "        plot_images([\n",
    "            (img, 'Original'),\n",
    "            (R_binary, 'R'),\n",
    "            (hls_binary, 'HLS'),\n",
    "            (hsv_binary, 'HSV'),\n",
    "            (lab_binary, 'LAB'),\n",
    "            (adapt_binary, 'Adaptive Thresh'),\n",
    "            (combined, 'Combined'),\n",
    "\n",
    "        ], figsize=(32, 42))\n",
    "\n",
    "    return  combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Demo\n",
    "Note:\"The figures will show in external when run the cell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 3 in plot_demo:\n",
    "    for img_path in test_img_paths[:2]: #video2[5:10]:\n",
    "        img, _ = get_image(img_path)\n",
    "        get_binary_image(img, visualise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
